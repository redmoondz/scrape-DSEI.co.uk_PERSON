import requests
import time
import re
from typing import List, Optional, Union, Dict, Any
from urllib.parse import urljoin, urlencode
from bs4 import BeautifulSoup, Tag
from tqdm import tqdm

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.settings import *
from src.models import Speaker, SpeakerSlug
from src.utils import (
    setup_logging, extract_slug_from_javascript, clean_text,
    parse_session_time, save_to_csv, delay_request
)


class DSEISpeakerScraper:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–ø–∏–∫–µ—Ä–æ–≤ —Å —Å–∞–π—Ç–∞ DSEI"""
    
    def __init__(self):
        self.logger = setup_logging(LOG_FILE)
        self.session = requests.Session()
        self.session.headers.update(HEADERS)
        self.speakers_slugs: List[SpeakerSlug] = []
        self.speakers_data: List[Speaker] = []
        
    def get_page(self, url: str, params: Optional[Dict[str, Any]] = None, retries: int = MAX_RETRIES) -> Optional[BeautifulSoup]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç BeautifulSoup"""
        for attempt in range(retries):
            try:
                self.logger.info(f"–ó–∞–ø—Ä–æ—Å –∫ {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries})")
                
                response = self.session.get(
                    url, 
                    params=params, 
                    timeout=REQUEST_TIMEOUT
                )
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                self.logger.info(f"–£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {response.url}")
                return soup
                
            except requests.RequestException as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {url}: {e}")
                if attempt < retries - 1:
                    delay_request(2 ** attempt)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                else:
                    self.logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ—Å–ª–µ {retries} –ø–æ–ø—ã—Ç–æ–∫")
                    
        return None
    
    def extract_speakers_slugs_from_page(self, soup: BeautifulSoup) -> List[SpeakerSlug]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ slug —Å–ø–∏–∫–µ—Ä–æ–≤ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        slugs = []
        seen_slugs = set()  # –î–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –ø–æ slug
        
        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å JavaScript —Ñ—É–Ω–∫—Ü–∏–µ–π openRemoteModal
        speaker_links = soup.find_all('a')
        
        for link in speaker_links:
            if not isinstance(link, Tag):
                continue
                
            href = link.get('href')
            if not href or 'openRemoteModal' not in str(href) or 'speakers/' not in str(href):
                continue
                
            slug = extract_slug_from_javascript(str(href))
            
            if slug and slug not in seen_slugs:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ slug
                seen_slugs.add(slug)
                
                # –ü–æ–ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–º—è —Å–ø–∏–∫–µ—Ä–∞
                name = ""
                # –ò—â–µ–º –≤ aria-label
                aria_label = link.get('aria-label')
                if aria_label:
                    name = clean_text(str(aria_label))
                else:
                    # –ò—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Å—Å—ã–ª–∫–∏ –∏–ª–∏ —Ä—è–¥–æ–º
                    text_content = link.get_text(strip=True)
                    if text_content:
                        name = clean_text(text_content)
                
                speaker_slug = SpeakerSlug(slug=slug, name=name)
                slugs.append(speaker_slug)
                self.logger.info(f"–ù–∞–π–¥–µ–Ω —Å–ø–∏–∫–µ—Ä: {name} (slug: {slug})")
        
        return slugs
    
    def check_for_next_page(self, soup: BeautifulSoup, current_page: int) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –µ—Å—Ç—å –ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å page= –≤ href
        all_links = soup.find_all('a', href=True)
        page_numbers = []
        
        for link in all_links:
            href = link.get('href', '')
            if 'page=' in href:
                try:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ href
                    page_num = int(href.split('page=')[1].split('&')[0])
                    page_numbers.append(page_num)
                except (ValueError, IndexError):
                    continue
        
        if page_numbers:
            max_page = max(page_numbers)
            self.logger.info(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {max_page}, —Ç–µ–∫—É—â–∞—è: {current_page}")
            return current_page < max_page
        
        return False
    
    def scrape_speakers_list(self) -> List[SpeakerSlug]:
        """–≠—Ç–∞–ø 1: –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö —Å–ø–∏–∫–µ—Ä–æ–≤ –∏ –∏—Ö slug"""
        self.logger.info("=== –≠–¢–ê–ü 1: –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Å–ø–∏–∫–µ—Ä–æ–≤ ===")
        
        all_slugs = []
        seen_slugs = set()  # –ì–ª–æ–±–∞–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
        page = 1
        
        while True:
            self.logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
            params = SPEAKERS_LIST_PARAMS.copy()
            params['page'] = str(page)
            
            soup = self.get_page(SPEAKERS_LIST_URL, params)
            if not soup:
                self.logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}")
                break
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º slug —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_slugs = self.extract_speakers_slugs_from_page(soup)
            
            if not page_slugs:
                self.logger.info(f"–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Å–ø–∏–∫–µ—Ä–æ–≤. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ.")
                break
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
            new_slugs = []
            for speaker_slug in page_slugs:
                if speaker_slug.slug not in seen_slugs:
                    seen_slugs.add(speaker_slug.slug)
                    new_slugs.append(speaker_slug)
            
            all_slugs.extend(new_slugs)
            self.logger.info(f"–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} –Ω–∞–π–¥–µ–Ω–æ {len(page_slugs)} —Å–ø–∏–∫–µ—Ä–æ–≤ ({len(new_slugs)} –Ω–æ–≤—ã—Ö)")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
            if not self.check_for_next_page(soup, page):
                self.logger.info("–î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
                break
                
            page += 1
            delay_request(DELAY_BETWEEN_REQUESTS)
        
        self.logger.info(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(all_slugs)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ø–∏–∫–µ—Ä–æ–≤")
        return all_slugs
    
    def extract_speaker_details(self, soup: BeautifulSoup, slug: str) -> Speaker:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–ø–∏–∫–µ—Ä–µ"""
        speaker = Speaker()
        speaker.speaker_slug = slug
        speaker.speaker_url = f"{SPEAKER_DETAIL_URL}/{slug}"
        
        # –ò–º—è —Å–ø–∏–∫–µ—Ä–∞
        title_elem = soup.find('h2', class_='m-speaker-entry__item__title')
        if title_elem:
            speaker.name = clean_text(title_elem.get_text())
        
        # –î–µ—Ç–∞–ª–∏ (–ø–æ–∑–∏—Ü–∏—è, –∫–æ–º–ø–∞–Ω–∏—è, —Å—Ç—Ä–∞–Ω–∞)
        details_elem = soup.find('div', class_='m-speaker-entry__item__details')
        if details_elem and isinstance(details_elem, Tag):
            # –ü–æ–∑–∏—Ü–∏—è
            position_elem = details_elem.find('span', class_='m-speaker-entry__item__details__position')
            if position_elem:
                speaker.position = clean_text(position_elem.get_text()).rstrip(',')
            
            # –ö–æ–º–ø–∞–Ω–∏—è
            company_elem = details_elem.find('span', class_='m-speaker-entry__item__details__company')
            if company_elem:
                speaker.company = clean_text(company_elem.get_text())
            
            # –°—Ç—Ä–∞–Ω–∞
            country_elem = details_elem.find('div', class_='m-speaker-entry__item__details__company__country')
            if country_elem:
                speaker.country = clean_text(country_elem.get_text())
        
        # –û–ø–∏—Å–∞–Ω–∏–µ
        description_elem = soup.find('div', class_='m-speaker-entry__item__description')
        if description_elem:
            # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç, –æ—á–∏—â–∞–µ–º –æ—Ç HTML —Ç–µ–≥–æ–≤
            description_text = description_elem.get_text()
            speaker.description = clean_text(description_text)
        
        # –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏
        social_list = soup.find('ul', class_='m-speaker-entry__item__social')
        if social_list and isinstance(social_list, Tag):
            social_links = []
            for social_item in social_list.find_all('li'):
                if isinstance(social_item, Tag):
                    link = social_item.find('a')
                    if link and isinstance(link, Tag):
                        href = link.get('href')
                        if href:
                            social_links.append(str(href))
            speaker.social_network = '; '.join(social_links)
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–µ—Å—Å–∏—è—Ö
        sessions_info = self.extract_session_info(soup)
        if sessions_info:
            speaker.session_date = sessions_info.get('date', '')
            speaker.session_time = sessions_info.get('time', '')
            speaker.session_location = sessions_info.get('location', '')
            speaker.session_topic_link = sessions_info.get('topic_link', '')
            speaker.session_topic_title = sessions_info.get('topic_title', '')
        
        return speaker
    
    def extract_session_info(self, soup: BeautifulSoup) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ—Å—Å–∏—è—Ö"""
        session_info = {
            'date': '',
            'time': '',
            'location': '',
            'topic_link': '',
            'topic_title': ''
        }
        
        # –î–∞—Ç–∞ —Å–µ—Å—Å–∏–∏
        date_elem = soup.find('div', class_='m-speaker-entry__item__sessions__list__item__date')
        if date_elem:
            session_info['date'] = clean_text(date_elem.get_text())
        
        # –í—Ä–µ–º—è —Å–µ—Å—Å–∏–∏  
        time_elem = soup.find('div', class_='m-speaker-entry__item__sessions__list__item__time')
        if time_elem:
            session_info['time'] = parse_session_time(time_elem.get_text())
        
        # –õ–æ–∫–∞—Ü–∏—è
        location_elem = soup.find('div', class_='m-speaker-entry__item__details__location')
        if location_elem:
            session_info['location'] = clean_text(location_elem.get_text())
        
        # –°—Å—ã–ª–∫–∞ –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã
        topic_link_elem = soup.find('a', class_='m-speaker-entry__item__sessions__list__item__title')
        if topic_link_elem and isinstance(topic_link_elem, Tag):
            href = topic_link_elem.get('href')
            if href:
                href_str = str(href)
                if href_str and not href_str.startswith('http'):
                    session_info['topic_link'] = urljoin(BASE_URL, href_str)
                else:
                    session_info['topic_link'] = href_str
            
            session_info['topic_title'] = clean_text(topic_link_elem.get_text())
        
        return session_info
    
    def scrape_speaker_details(self, speaker_slugs: List[SpeakerSlug]) -> List[Speaker]:
        """–≠—Ç–∞–ø 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –∫–∞–∂–¥–æ–º—É —Å–ø–∏–∫–µ—Ä—É"""
        self.logger.info("=== –≠–¢–ê–ü 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ===")
        
        speakers = []
        
        for speaker_slug in tqdm(speaker_slugs, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏–∫–µ—Ä–æ–≤"):
            self.logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏–∫–µ—Ä–∞: {speaker_slug.slug}")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            detail_url = f"{SPEAKER_DETAIL_URL}/{speaker_slug.slug}"
            
            soup = self.get_page(detail_url, SPEAKER_DETAIL_PARAMS)
            if not soup:
                self.logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–ø–∏–∫–µ—Ä–∞ {speaker_slug.slug}")
                continue
            
            speaker = self.extract_speaker_details(soup, speaker_slug.slug)
            
            # –ï—Å–ª–∏ –∏–º—è –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∏–∑ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑ —Å–ø–∏—Å–∫–∞
            if not speaker.name and speaker_slug.name:
                speaker.name = speaker_slug.name
            
            speakers.append(speaker)
            self.logger.info(f"–î–∞–Ω–Ω—ã–µ —Å–ø–∏–∫–µ—Ä–∞ {speaker.name} —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω—ã")
            
            delay_request(DELAY_BETWEEN_REQUESTS)
        
        return speakers
    
    def run(self) -> None:
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        self.logger.info("=== –ù–ê–ß–ê–õ–û –ü–ê–†–°–ò–ù–ì–ê –°–ü–ò–ö–ï–†–û–í DSEI ===")
        start_time = time.time()
        
        try:
            # –≠—Ç–∞–ø 1: –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Å–ø–∏–∫–µ—Ä–æ–≤
            self.speakers_slugs = self.scrape_speakers_list()
            
            if not self.speakers_slugs:
                self.logger.error("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
                return
            
            # –≠—Ç–∞–ø 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            self.speakers_data = self.scrape_speaker_details(self.speakers_slugs)
            
            if not self.speakers_data:
                self.logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∏ –ø–æ –æ–¥–Ω–æ–º—É —Å–ø–∏–∫–µ—Ä—É.")
                return
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            speakers_dict = [speaker.to_dict() for speaker in self.speakers_data]
            save_to_csv(speakers_dict, SPEAKERS_CSV_FILE)
            
            # –ò—Ç–æ–≥–∏
            end_time = time.time()
            duration = end_time - start_time
            
            self.logger.info("=== –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù ===")
            self.logger.info(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Å–ø–∏–∫–µ—Ä–æ–≤: {len(self.speakers_slugs)}")
            self.logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(self.speakers_data)}")
            self.logger.info(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.2f} —Å–µ–∫—É–Ω–¥")
            self.logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {SPEAKERS_CSV_FILE}")
            
            print(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")
            print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–ø–∏–∫–µ—Ä–æ–≤: {len(self.speakers_data)}")
            print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {SPEAKERS_CSV_FILE}")
            print(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.2f} —Å–µ–∫")
            
        except Exception as e:
            self.logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}", exc_info=True)
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        
        finally:
            self.session.close()
